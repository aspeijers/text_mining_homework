{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 0 HW2\n",
    "\n",
    "1. This assignment is an individual effort.\n",
    "2. Submission to be uploaded into your group repositories in the folder python_intro\n",
    "3. Deadline is 20th of April 5:00 PM.\n",
    "4. Please follow google's [python styleguide](https://google.github.io/styleguide/pyguide.html) for your code. Pay attention to the guidelines for naming convention, comments and main.\n",
    "5. Code will be checked for plagiarism. Compelling signs of a duplicated effort will lead to a rejection of submission and will attract a 100\\% grade penalty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "Please load the file provided to you by email. Use _json_ module to read it as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 use open a connection to the file\n",
    "# 2 read contents of the file\n",
    "# 3 use the json module to read the string as a list\n",
    "\n",
    "# import json module\n",
    "import json\n",
    "\n",
    "# create link to file, read as a string, convert to a list\n",
    "file_handle = open(\"1948_Truman_anneke.txt\")\n",
    "file_content = file_handle.read()\n",
    "speech = json.loads(file_content)\n",
    "\n",
    "\n",
    "# Notes\n",
    "# - since lists are mutable, if we want to define a new list based on the original one and then update it we need to \n",
    "# do the following:\n",
    "#     newspeech = list(speech)\n",
    "# - then if we check the id's they will be different:\n",
    "#     print id(speech), id(newspeech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that type(speech) is list. Please take a moment to go through the python list documentation and check out the various ways to manipulate lists.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2\n",
    "The first element of the list is the year of speech, the second element is the president who gave it, while the third is the transcript of the same. \n",
    "\n",
    "1. Inspect the transcript. Note the commonly used non-alphanumerical characters. Use an appropriate method of strings to get rid of them.\n",
    "2. Use an appropriate string method to split the string of the speech into a list of smaller list of words.\n",
    "3. Convert all words into lower case and return the list. Use a for loop. Then use a list comprehension to do the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 1 \n",
    "# import re module\n",
    "import re\n",
    "\n",
    "# substitute all characters that don't match (^) space (/s) or alphanumeric (/w) with a space in the actual speech. \n",
    "# Note that '_' is included included in the set of alphanumeric characters, so need to match and replace them as well. \n",
    "stripped_text = re.sub(r'([^\\s\\w]|_)+', ' ', speech[2])\n",
    "\n",
    "\n",
    "## 2\n",
    "# split text based on spaces\n",
    "word_list = stripped_text.split()\n",
    "\n",
    "\n",
    "## 3 \n",
    "\"\"\"\n",
    "The following ways two ways of writing the for loop are equivalent\n",
    "square = []\n",
    "num_list = [1, 2, 3, 4]\n",
    "for num in num_list:\n",
    "    square.append(num**2)\n",
    "num_square = [num**2 for num in num_list]\n",
    "print square, num_square\n",
    "\"\"\"\n",
    "# as a for loop\n",
    "lower_words = list()\n",
    "for index in range(len(word_list)):\n",
    "    word = word_list[index]\n",
    "    lower_words.append(word.lower())\n",
    "\n",
    "# as a list comprehension\n",
    "lower_words = [word.lower() for word in word_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "Create a function _preprocess_ that takes as arguments _text_ and _non_alphanum_, a string of non-alphanumeric characters that you want get rid of. Perform all operations specfied in the previous question. However, converting to lowercase should be an optional argument. The data structure returned should be a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inputs: text: a string, nonalphanum: a string of non-alphanumeric characters to remove, lower: True/False whether to transform to lower case. \n",
    "# Output: list of individual words from the text.\n",
    "\n",
    "def preprocess(text, nonalphanum, lower = True):\n",
    "    \n",
    "    # define regex\n",
    "    regex = \"([\" + nonalphanum + \"]|_)+\"\n",
    "    \n",
    "    # replace non-alphanumeric characters with a space\n",
    "    stripped_text = re.sub(regex, ' ', text)\n",
    "    word_list = stripped_text.split()\n",
    "    if lower == True:\n",
    "        words = [word.lower() for word in word_list]\n",
    "    else:\n",
    "        words = word_list\n",
    "    \n",
    "    return words\n",
    "\n",
    "# call preprocess function\n",
    "# print preprocess(speech[2], \".:\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "Create a function _word_freq_ that takes as input a word list that has been preprocessed and returns a dictionary of the word frequency. Which is the fourth most frequent word of your word list? (Provide code that computes it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n"
     ]
    }
   ],
   "source": [
    "#Input: preprocessed word list\n",
    "#Output: a dictionary  of the word frequency\n",
    "\n",
    "def wordfreq(preprocessed_words):\n",
    "    counts = dict()\n",
    "    for word in preprocessed_words:\n",
    "        counts[word] = counts.get(word,0) + 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "## Find 4th most frequent word\n",
    "frequencies = wordfreq(lower_words)\n",
    "\n",
    "# Using list comprehension - create a list of tuples from the dictionary and sort by the value.\n",
    "sorted_freq = sorted([(val, key) for key, val in frequencies.items()])\n",
    "\n",
    "# find the 4th most frequent and take the key\n",
    "print sorted_freq[-4][1]\n",
    "\n",
    "\n",
    "# # Could also do this using a for loop\n",
    "# lst = list()\n",
    "# for key, val in frequencies.items():\n",
    "#     lst.append((val,key))\n",
    "\n",
    "# sorted_freq = sorted(lst)\n",
    "# print sorted_freq[-4][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5\n",
    "Write a function that takes as input a word list and returns a dictionary of the frequencies of word lengths. Do not use the api collections for this assignment. But have a look at its [documentation](https://docs.python.org/2/library/collections.html). Its useful tool to have in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accomplishments\n",
      "accomplishments\n",
      "discrimination\n",
      "inconsistencies\n",
      "responsibility\n",
      "responsibilities\n",
      "administration\n",
      "electrification\n",
      "constitutional\n",
      "representatives\n",
      "responsibilities\n",
      "reconstruction\n",
      "reconstruction\n",
      "reconstruction\n",
      "reconstruction\n",
      "extraordinarily\n",
      "{1: 112, 2: 989, 3: 1018, 4: 728, 5: 521, 6: 450, 7: 432, 8: 321, 9: 229, 10: 172, 11: 85, 12: 46, 13: 21, 14: 8, 15: 6, 16: 2}\n"
     ]
    }
   ],
   "source": [
    "#Input: preprocessed word list\n",
    "#Output: a dictionary of the frequencies of word lengths\n",
    "\n",
    "def wordlengths(preprocessed_words):\n",
    "    counts = {}\n",
    "    for word in preprocessed_words:\n",
    "        length = len(word)\n",
    "        counts[length] = counts.get(length,0) + 1\n",
    "    return counts\n",
    "\n",
    "# call wordlengths function\n",
    "# print wordlengths(lower_words)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6\n",
    "Load the file _sou_all.txt_ in ./data/pres_speech. Inspect its contents. Familiarise yourself with using regular expressions in python. You can use this [document](https://docs.python.org/2/howto/regex.html) as a starting point. Now use regular expressions to seperate the different speeches. Your function should accept the text and a regular expression as input and return a list of lists. Each element of the list should be a list with following structure:\n",
    "\n",
    "1. year\n",
    "2. president\n",
    "3. List of the transcript of the speech broken down into paragraphs.\n",
    "\n",
    "Save your result as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create nested list\n",
    "# create link to file and read\n",
    "file_handle1 = open(\"sou_all.txt\")\n",
    "file_content1 = file_handle1.read().decode(\"utf-8\")\n",
    "\n",
    "# split by speech and remove first element of resulting list which is empty\n",
    "speeches = re.compile(\"[*]{10,}\").split(file_content1)\n",
    "del speeches[0]\n",
    "\n",
    "# create nested list for each speech\n",
    "for speech in range(len(speeches)):\n",
    "    # get the speech\n",
    "    original = speeches[speech]\n",
    "    \n",
    "    # take out the year\n",
    "    year = original[1:5]\n",
    "    remaining = original[5:]\n",
    "    \n",
    "    # take out president\n",
    "    president_pattern = re.compile(\"[*]{5,}\").split(remaining)\n",
    "    president = \" \".join(re.findall(\"[A-Z][^_]*\", president_pattern[0]))\n",
    "    \n",
    "    # split remaining speech into paragraphs\n",
    "    paragraphs = re.compile('[\\n]*').split(president_pattern[1])[1:-1]\n",
    "    #del paragraphs[0]\n",
    "    #del paragraphs[-1] - could also do this\n",
    "                    \n",
    "    # redefine the list of speeches\n",
    "    speeches[speech] = [ year, president, paragraphs]\n",
    "\n",
    "\n",
    "## Write list to json file\n",
    "import io\n",
    "\n",
    "# json.dumps() converts nested list to string\n",
    "speechesJSON = json.dumps(speeches, ensure_ascii=False)\n",
    "\n",
    "# open connection to file (which hasn't been created yet) and then write json file. \n",
    "# nb. We would usually use json.dump() but there is a bug relating to utf8 encoding so we use the io library.\n",
    "with io.open('speeches.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(unicode(speechesJSON))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
